@article{Iyengar2005,
    author = {Iyengar, Garud N.},
    journal = {Mathematics of Operations Research},
    pages = {257--280},
    title = {{ Robust Dynamic Programming }},
    volume = {30},
    year = {2005}
}

@article{Nilim2005,
    author = {Nilim, Arnab and {El Ghaoui} , Laurent},
    journal = {Operations Research},
    pages = {780--798},
    title = {{ Robust Control of Markov Decision Processes with Uncertain Transition Matrices }},
    volume = {53},
    year = {2005}
}

@article{Wiesemann2013,
    author = {Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Ber√ß},
    journal = {Mathematics of Operations Research},
    pages = {1--52},
    title = {{ Robust Markov Decision Processes }},
    year = {2013}
}

@article{Garcia2015,
    author = {Garc{\'{i}}a, Javier and Fern{\'{a}}ndez, Fernando},
    issn = {15337928},
    journal = {Journal of Machine Learning Research},
    pages = {1437--1480},
    title = {{ A Comprehensive Survey on Safe Reinforcement Learning }},
    volume = {16},
    year = {2015}
}

@article{Tamar2012,
    arxivId = {1301.0104},
    author = {Tamar, Aviv and { Di Castro } , Dotan and Mannor, Shie},
    journal = {International Conference on Machine Learning},
    pages = {935--942},
    title = {{ Policy Gradients with Variance Related Risk Criteria }},
    year = {2012}
}

@incollection{Chow2014,
author = {Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {Cortes, C and Lawrence, N D and Lee, D D and Sugiyama, M and Garnett, R},
pages = {1522--1530},
publisher = {Curran Associates, Inc.},
title = {{Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach}},
url = {http://papers.nips.cc/paper/6014-risk-sensitive-and-robust-decision-making-a-cvar-optimization-approach.pdf},
year = {2015}
}

@article{Carrara2018,
    author = {Carrara, Nicolas and Laroche, Romain and Bouraoui, Jean-L{\'{e}}on and Urvoy, Tanguy and Pietquin, Olivier},
    title = {{ A Fitted-Q Algorithm for Budgeted MDPs }},
    journal = {UAI2018, Workshop on Safety, Risk and Uncertainty in Reinforcement Learning},
    year = {2018}
}

@article{Leurent2018,
    author = {Leurent, Edouard and Blanco, Yann and Efimov, Denis and Maillard, Odalric-Ambrym},
    journal = {NeurIPS 2018, Workshop on Machine Learning for Intelligent Transportation Systems},
    title = {{ Approximate Robust Control of Uncertain Dynamical Systems }},
    year = {2018}
}

@inproceedings{Riedmiller2005,
author = {Riedmiller, Martin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11564096_32},
isbn = {3540292438},
issn = {03029743},
pmid = {12609028},
title = {{Neural fitted Q iteration - First experiences with a data efficient neural Reinforcement Learning method}},
year = {2005}
}

@article{Carrara2018b,
    title = {{Safe transfer learning for dialogue applications}},
    author = {Carrara, Nicolas and Laroche, Romain and Bouraoui, Jean-L{\'{e}}on and Urvoy, Tanguy and Pietquin, Olivier},
    journal = {SLSP 2018 - 6th International Conference on Statistical Language and Speech Processing},
    year = {2018},
}

@article{Khouzaimi2015,
    author = {Khouzaimi, Hatim  and Laroche, Romain and Lefevre, Fabrice.},
    title = {{ Optimising turn-taking strategies with reinforcement learning. }},
    journal = {Sigdial 2015 - 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
    year = {2016}
}


@inproceedings{Abe:2010:ODC:1835804.1835817,
 author = {Abe, Naoki and Melville, Prem and Pendus, Cezar and Reddy, Chandan K. and Jensen, David L. and Thomas, Vince P. and Bennett, James J. and Anderson, Gary F. and Cooley, Brent R. and Kowalczyk, Melissa and Domick, Mark and Gardinier, Timothy},
 title = {Optimizing Debt Collections Using Constrained Reinforcement Learning},
 booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)},
 year = {2010}
 }
 
 @article{ChunmingLiu2014,
author = {{Chunming Liu} and {Xin Xu} and {Dewen Hu}},
doi = {10.1109/tsmc.2014.2358639},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title = {{Multiobjective Reinforcement Learning: A Comprehensive Overview}},
year = {2014}
}

@misc{Bertsekas1999,
author = {Bertsekas, D.P.},
booktitle = {Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, J. Neyman , University of California Press, Berkeley,},
doi = {10.1137/1.9780898719383},
isbn = {1886529000},
issn = {0025-5610},
pmid = {2091171},
title = {{Nonlinear programming}},
year = {1999}
}


@article{BEUTLER1985236,
title = "Optimal policies for controlled Markov chains with a constraint",
journal = "Journal of Mathematical Analysis and Applications",
volume = "112",
number = "1",
pages = "236 - 252",
year = "1985",
issn = "0022-247X",
doi = "https://doi.org/10.1016/0022-247X(85)90288-4",
url = "http://www.sciencedirect.com/science/article/pii/0022247X85902884",
author = "Frederick J. Beutler and Keith W. Ross",
abstract = "The time average reward for a discrete-time controlled Markov process subject to a time-average cost constraint is maximized over the class of al causal policies. Each epoch, a reward depending on the state and action, is earned, and a similarly constituted cost is assessed; the time average of the former is maximized, subject to a hard limit on the time average of the latter. It is assumed that the state space is finite, and the action space compact metric. An accessibility hypothesis makes it possible to utilize a Lagrange multiplier formulation involving the dynamic programming equation, thus reducing the optimization problem to an unconstrained optimization parametrized by the multiplier. The parametrized dynamic programming equation possesses compactness and convergence properties that lead to the following: If the constraint can be satisfied by any causal policy, the supremum over time-average rewards respective to all causal policies is attained by either a simple or a mixed policy; the latter is equivalent to choosing independently at each epoch between two specified simple policies by the throw of a biased coin."
}

@article{DBLP:journals/corr/AchiamHTA17,
  author    = {Joshua Achiam and
               David Held and
               Aviv Tamar and
               Pieter Abbeel},
  title     = {Constrained Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1705.10528},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.10528},
  archivePrefix = {arXiv},
  eprint    = {1705.10528},
  timestamp = {Wed, 07 Jun 2017 14:41:44 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/AchiamHTA17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Graham1972,
  author = {Graham, Ronald L.},
  journal = {Inf. Process. Lett.},
  title = {An Efficient Algorithm for Determining the Convex Hull of a Finite Planar Set},
  year = 1972
}

@inproceedings{Li2009ReinforcementLF,
  title={Reinforcement learning for dialog management using least-squares Policy iteration and fast feature selection},
  author={Lihong Li and Jason D. Williams and Suhrid Balakrishnan},
  booktitle={Proceedings of the Conference of the International Speech Communication Association (Interspeech)},
  year={2009}
}

@inproceedings{chandramohan2010optimizing,
  title={Optimizing spoken dialogue management with fitted value iteration},
  author={Chandramohan, Senthilkumar and Geist, Matthieu and Pietquin, Olivier},
  booktitle={Proceedings of the Conference of the International Speech Communication Association (Interspeech)},
  year={2010}
}

@inproceedings{Boutilier_Lu:uai16,
       author = "Craig Boutilier and Tyler Lu",
       title = "Budget Allocation using Weakly Coupled, Constrained Markov Decision Processes",
       booktitle = "Proceedings of the 32nd Conference on Uncertainty in Artificial Intelligence (UAI)",
       year = "2016",
 }


 @book{Altman95constrainedmarkov,
     author = {Eitan Altman},
     title = {Constrained Markov Decision Processes},
     year = {1999},
    publisher={CRC Press}
 }

 @article{Bellman,
   title={Dynamic programming and Lagrange multipliers},
   author={Bellman, Richard},
   journal={Proceedings of the National Academy of Sciences of the United States of America},
   year={1956}
 }

 @book{1886529043,
    author = {Bertsekas, Dimitri P.},
   title = {Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural Computation Series)},
   year = 1996,
publisher = {Athena Scientific},
 }

 @inproceedings{carrara:hal-01557775,
   TITLE = {{Online learning and transfer for user adaptation in dialogue systems}},
   AUTHOR = {Carrara, Nicolas and Laroche, Romain and Pietquin, Olivier},
   booktitle = {{SIGDIAL/SEMDIAL joint special session on negotiation dialog 2017}},
   YEAR = {2017},
 }

@article{DBLP:journals/corr/ChowGJP15,
  author    = {Yinlam Chow and
               Mohammad Ghavamzadeh and
               Lucas Janson and
               Marco Pavone},
  title     = {Risk-Constrained Reinforcement Learning with Percentile Risk Criteria},
  journal   = {CoRR},
  volume    = {abs/1512.01629},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.01629},
  archivePrefix = {arXiv},
  eprint    = {1512.01629},
  timestamp = {Wed, 07 Jun 2017 14:40:55 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/ChowGJP15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

 @article{slsqp,
   author    = {Dieter Kraft and Klaus Schnepper},
   title     = {SLSQP, a Nonlinear Programming Method with Quadratic Programming Subproblems},
   journal   = {DLR, Oberpfaffenhofen},
   year      = {1989}
 }

 @article{Ernst2005,
 author = {Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
 journal = {Journal of Machine Learning Research},
 title = {{Tree-Based Batch Mode Reinforcement Learning}},
 year = {2005}
 }

 @article{Geibel2005,
  title={Risk-sensitive reinforcement learning applied to control under constraints.},
  author={Geibel, Peter and Wysotzki, Fritz},
  journal={Journal of Artificial Intelligence},
  volume={24},
  pages={81--108},
  year={2005}
}

 @inproceedings{Genevay2016,
   title={Transfer Learning for User Adaptation in Spoken Dialogue Systems},
   author={Genevay, Aude and Laroche, Romain},
   booktitle={Proceedings of the 15th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)},
   year={2016},
 }
 
@article{Mnih2015,
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
year = {2015}
}



 @article{Hoerl:2000:RRB:338441.338461,
  author = {Hoerl, Arthur E. and Kennard, Robert W.},
  title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
  journal = {Technometrics},
  year = {2000}
 }

 @article{Laroche2017,
    author = {{Laroche}, Romain and {Trichelair}, Paul},
     title = "{Safe Policy Improvement with Baseline Bootstrapping}",
   journal = {CoRR},
      year = 2017
}

@incollection{LazaricSurvey,
  TITLE = {{Transfer in Reinforcement Learning: a Framework and a Survey}},
  AUTHOR = {Lazaric, Alessandro},
  URL = {https://hal.inria.fr/hal-00772626},
  BOOKTITLE = {{Reinforcement Learning - State of the art}},
  EDITOR = {Marco Wiering, Martijn van Otterlo},
  PUBLISHER = {{Springer}},
  VOLUME = {12},
  PAGES = {143-173},
  YEAR = {2012},
  DOI = {10.1007/978-3-642-27645-3\_5},
  PDF = {https://hal.inria.fr/hal-00772626/file/transfer.pdf},
  HAL_ID = {hal-00772626},
  HAL_VERSION = {v1},
}

@inproceedings{Massoud2009,
author = {Massoud, Amir and Ghavamzadeh, Farahmand Mohammad and Szepesv{\'{a}}ri, Csaba and Mannor, Shie},
title = {{Regularized Fitted Q-iteration for Planning in Continuous-Space Markovian Decision Problems}},
year={2009},
booktitle={Proceedings of the American Control Conference (ACC)}
}

@inproceedings{Petrik2016,
  title={Safe policy improvement by minimizing robust baseline regret},
  author={Petrik, Marek Ghavamzadeh, Mohammad and and Chow, Yinlam},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2016}
}

@inproceedings{Santana:2016:RAC:3016100.3016366,
 author = {Santana, Pedro and Thi{\'e}baux, Sylvie and Williams, Brian},
 title = {RAO*: An Algorithm for Chance-constrained POMDP's},
 booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI)},
 year = {2016}
}

@book{SuttonBarto,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{Taylor2009,
author = {Taylor, Matthew E and Stone, Peter},
journal = {Journal of Machine Learning Research},
title = {{Transfer Learning for Reinforcement Learning Domains : A Survey}},
year = {2009}
}

@inproceedings{Thomas2015,
  title={High confidence policy improvement},
  author={Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  year={2015}
}

@article{Tikhonov1963,  
  title={Regularization of incorrectly posed problems},
  author={Tikhonov, Andrei Nikolaevich},
  journal={Doklady Akademii Nauk SSSR},
  year={1963}
}

% pas de conf ni d'annee
@techreport{Undurti,
    title = {{Function Approximation for Continuous Constrained MDPs}},
    author = {Undurti, Aditya and Geramifard, Alborz and Roy, Nicholas and How, Jonathan P},
    year={2010}
}

@inproceedings{Undurti2010,
    title = {{An online algorithm for constrained POMDPs}},
    year = {2010},
    booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
    author = {Undurti, Aditya and How, Jonathan P.},
}


@inproceedings{carrara2018safe,
    title = {Safe transfer learning for dialogue applications},
    author = {Carrara, Nicolas and Laroche, Romain and Bouraoui, Jean-L√©on and Urvoy, Tanguy and Pietquin, Olivier},
    booktitle = {6th International Conference on Statistical Language and Speech Processing (SLSP 2018)},
    year = {2018},
    keywords = {me}
}

 @article{abs-1109-2147,
  title={Risk-sensitive reinforcement learning applied to control under constraints.},
  author={Geibel, Peter and Wysotzki, Fritz},
  journal={Journal of Artificial Intelligence},
  volume={24},
  year={2005}
}
 @article{ChowGJP15,
  author  = {Yinlam Chow and Mohammad Ghavamzadeh and Lucas Janson and Marco Pavone},
  title   = {Risk-Constrained Reinforcement Learning with Percentile Risk Criteria},
  journal = {JMLR},
  year    = {2018},
  url     = {http://jmlr.org/papers/v18/15-636.html}
}

@InProceedings{AchiamHTA17,
  title = 	 {Constrained Policy Optimization},
  author = 	 {Joshua Achiam and David Held and Aviv Tamar and Pieter Abbeel},
  booktitle = 	 {ICML},
  year = 	 {2017},
  url = 	 {http://proceedings.mlr.press/v70/achiam17a.html},
  abstract = 	 {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.}
}
 @inproceedings{Abe2010,
  author    = {Naoki Abe and others
},
  title     = {Optimizing debt collections using constrained reinforcement learning},
  booktitle = {SIGKDD},
  year      = {2010},
  url       = {http://doi.acm.org/10.1145/1835804.1835817},
  timestamp = {Sun, 04 Jun 2017 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/kdd/AbeMPRJTBACKDG10},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

 @book{bertsekas1996,
    author = {Bertsekas, Dimitri P.},
   title = {Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural Computation Series)},
   year = 1996,
publisher = {Athena Scientific},
 }

@inproceedings{Poupart2015,
	author = {Pascal Poupart and Aarti Malhotra and Pei Pei and Kee-Eung Kim and Bongseok Goh and Michael Bowling},
	title = {Approximate Linear Programming for Constrained Partially Observable Markov Decision Processes},
	booktitle = {AAAI Conference on Artificial Intelligence},
	year = {2015},
	keywords = {Constrained POMDPs; Approximate Linear Programming; Finite State Controller},
	abstract = {In many situations, it is desirable to optimize a sequence of decisions by maximizing a primary objective while respecting some constraints with respect to secondary objectives. Such problems can be naturally modeled as constrained partially observable Markov decision processes (CPOMDPs) when the environment is partially observable. In this work, we describe a technique based on approximate linear programming to optimize policies in CPOMDPs. The optimization is performed offline and produces a finite state controller with desirable performance guarantees. The approach outperforms a constrained version of point-based value iteration on a suite of benchmark problems.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9742}
}

@misc{le2019batch,
    title={Batch Policy Learning under Constraints},
    author={Hoang M. Le and Cameron Voloshin and Yisong Yue},
    year={2019},
    eprint={1903.08738},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{Roijers2013ASO,
  title={A Survey of Multi-Objective Sequential Decision-Making},
  author={Diederik M. Roijers and Peter Vamplew and Shimon Whiteson and Richard Dazeley},
  journal={J. Artif. Intell. Res.},
  year={2013},
  volume={48},
  pages={67-113}
}