% \documentclass{article}

% % if you need to pass options to natbib, use, e.g.:
% %     \PassOptionsToPackage{numbers, compress}{natbib}
% % before loading neurips_2019

% % ready for submission
% % \usepackage{neurips_2019}

% % to compile a preprint version, e.g., for submission to arXiv, add add the
% % [preprint] option:
% %     \usepackage[preprint]{neurips_2019}

% % to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage{neurips_2019}

% % to avoid loading the natbib package, add option nonatbib:
% %     \usepackage[nonatbib]{neurips_2019}

% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography

% % Paper specific includes and commands
% \usepackage{times}
% \usepackage{soul}
% \usepackage[small]{caption}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{amsfonts}
% \usepackage{amsthm}
% \usepackage[shortlabels]{enumitem}
% \usepackage{algorithm}
% \usepackage{algorithmic}
% \usepackage{tabularx}
% \usepackage{makecell}
% \urlstyle{same}
% \usepackage{xargs}
% \usepackage{mathtools}
% \usepackage{dsfont}
% \usepackage{import}
% \usepackage{svg}
% \usepackage{pdfpages}

% \input{mathdef}


% \makeatletter
% \providecommand*{\input@path}{}
% \g@addto@macro\input@path{{./source/}{./source/ressources/}{./ressources/}}
% \makeatother

% \urlstyle{same}

% \usepackage{wrapfig}
% \usepackage{xr-hyper}
% \usepackage{hyperref}
% \usepackage[capitalise,noabbrev]{cleveref}

% % the following package is optional:
% %\usepackage{latexsym}

% \title{Supplementary material}

% \author{
% Nicolas Carrara$^{1,}$\thanks{equal contribution}\and
% Edouard Leurent$^{1,3,}$\footnotemark[1]\and
% Romain Laroche$^{4}$\and
% Tanguy Urvoy$^{2}$\and
% Odalric Maillard$^{1}$\and
% Olivier Pietquin$^{1,2,}$\thanks{now with Google Brain, Paris.}\\
% \affiliations $^1$SequeL team, INRIA Lille -- Nord Europe, France\\
% $^2$Orange Labs, Lannion, France\\
% $^3$Renault Group, France\\
% $^4$Microsoft Research , Montreal, Canada
% \emails
% \{nicolas.carrara, edouard.leurent, odalric.maillard, olivier.pietquin\}@inria.fr,
% romain.laroche@microsoft.com, tanguy.urvoy@orange.com
% }


% \begin{document}
% \maketitle

\begin{center}
\LARGE Supplementary material
\end{center}
\appendix

\section{Proofs of Main Results}
\label{sec:proofs}
\subsection{Proposition \ref{prop:bellman-expectation}}

\begin{proof}
\begin{align*}
    V^\pi(\os) &= \expectedvalue\left[ J^\pi \condbar \ov{s_0} = \os\right] \\
    &=\sum_{\oa\in\ocA} \probability{\oa_0 = \oa \condbar\ov{s_0} = \os} \expectedvalue\left[ J^\pi \condbar \ov{s_0} = \os, \oa_0 = \oa\right]\\
    &= \sum_{\oa\in\ocA} \pi(\oa | \os) Q^\pi(\os,\oa)
\end{align*}
\begin{align*}
    Q^\pi(\os, \oa) &= \expectedvalue\left[\sum_{t=0}^\infty \gamma^t R(\os_t, \oa_t)\condbar \ov{s_0} = \os, \ov{a_0} = \oa\right] \\
    &= R(\os, \oa) + \sum_{\os'\in\ocS}\probability{\os_1 = \os' \condbar\ov{s_0} = \os, \ov{a_0} = \oa}\expectedvalue\left[\sum_{t=1}^\infty \gamma^t R(\os_t, \oa_t)\condbar \ov{s_1} = \os'\right] \\
    &= R(\os, \oa) + \gamma\sum_{\os'\in\ocS}\ov{P}\left(\os' \condbar\os, \oa\right) \expectedvalue\left[\sum_{t=0}^\infty \gamma^t R(\os_t, \oa_t) \condbar \ov{s_0} = \os'\right] \\
    &=  R(\os, \oa) + \gamma\sum_{\os'\in\ocS}\ov{P}\left(\os' \condbar\os, \oa\right) V^\pi(\os')
\end{align*}
\end{proof}

\subsection{Theorem \ref{thm:bellman-optimality}}

\begin{proof}
Let $\os, \oa \in \ocA\times\ocS$. For this proof, we consider  potentially non-stationary policies $\pi=(\rho, \pi')$, with $\rho\in\cM(\ocA)$, $\pi'\in\cM(\ocA)^\Natural$. The results will apply to the particular case of stationary optimal policies, when they exist.

\begin{align}
    Q_r^*(\os, \oa) &=  \max_{\rho, \pi'} Q_r^{\rho, \pi'}(\os', \oa') \label{eq:pthm_def}\\
    &= \max_{\rho, \pi'} R_r(\os, \oa) + \gamma \sum_{\os'\in\cS} P(\os' | \os, \oa) V_r^{\rho, \pi'}(\os') \label{eq:pthm_exp}\\
    &= R_r(\os, \oa) + \gamma \sum_{\os'\in\cS}  P(\os' | \os, \oa) \max_{\rho, \pi'} \sum_{\oa'\in\ocA} \rho(\oa' | \os')Q_r^{\pi'}(\os', \oa') \label{eq:pthm_marg}\\
    &= R_r(\os, \oa) + \gamma \sum_{\os'\in\cS}  P(\os' | \os, \oa) \max_\rho\sum_{\oa'\in\ocA}\rho(\oa' | \os')\max_{\pi'\in\Pi_a(\os')}Q_r^{\pi'}(\os', \oa') \label{eq:pthm_max}\\
    &= R_r(\os, \oa) + \gamma \sum_{\os'\in\cS}  P(\os' | \os, \oa) \max_\rho\expectedvalueover{\oa'\sim\rho}Q_r^*(\os', \oa') \label{eq:pthm_marg_def2}
\end{align}
where $\pi = (\rho, \pi')\in\Pi_a(\os)$ and $\pi'\in\Pi_a(\os')$.

This follows from:
\begin{enumerate}
\item[\eqref{eq:pthm_def}.] Definition of $Q^*$. 
\item[\eqref{eq:pthm_exp}.] Bellman Expectation expansion from Proposition \ref{prop:bellman-expectation}.
\item[\eqref{eq:pthm_marg}.] Marginalisation on $\oa'$.
\item[\eqref{eq:pthm_max}.] \begin{itemize}
    \item Trivially $\max_{\pi'\in\Pi_a(\os')} \sum_{\oa'\in\cA} \cdot \leq \sum_{\oa'\in\cA} \max_{ \pi'\in\Pi_a(\os)} \cdot$
    \item Let $\ov{\pi}\in\argmax_{\pi'\in\Pi_a(\os')} Q_r^{\pi'}(\os', \oa')$, then:
    \begin{align*}
        \sum_{\oa'\in\ov{A}}\rho(\oa'|\os')\max_{\pi'\in\Pi_a(\os')}Q_r^{\pi'}(\os', \oa') &= \sum_{\oa'\in\ov{A}}\rho(\oa'|\os')Q_r^{\ov{\pi}}(\os', \oa') \\
        &\leq  \max_{\pi'\in\Pi_a(\os')} \sum_{\oa'\in\ov{A}}\rho(\oa'|\os')Q_r^{\pi'}(\os', \oa')
    \end{align*}
\end{itemize}
\item[\eqref{eq:pthm_marg_def2}.] Definition of $Q^*$.
\end{enumerate}

Moreover, the condition $\pi=(\rho, \pi')\in\Pi_a(\os)$ gives
\begin{equation*}
   \expectedvalueover{\oa'\sim\rho} Q_c^{*}(\os, \oa) = \expectedvalueover{\oa'\sim\rho} Q_c^{\pi'}(\os, \oa) = V_c^{\pi}(\os) \leq \beta
\end{equation*}

Consequently, $\pi_\text{greedy}(\cdot; Q^*)$ belongs to the $\argmax$ of \eqref{eq:pthm_marg_def2}, and in particular:
\begin{equation*}
     Q_r^*(\os, \oa) = r(\os, \oa) + \gamma \sum_{\os'\in\cS}  P(\os' | \os, \oa) \expectedvalueover{\oa'\sim\pi_\text{greedy}(\os', Q^*)} Q_r^*(\os', \oa')
\end{equation*}

The same reasoning can be made for $Q_c^*$ by replacing $\max$ operators by $\min$, and $\Pi_a$ by $\Pi_r$.
\end{proof}

\subsection{Proposition \ref{prop:greedy_optimal}}

\begin{definition}
We define the Bellman Expectation Operator $\cT^\pi$:\\
$\forall \pi\in\Pi, Q\in(\Real^2)^{\ocS\ocA}, \os\in\ocS, \oa\in\ocA$,
\begin{align}
\label{eq:bellman_exepctation_operator}
    \cT^\pi Q(\os, \oa) &= R(\os, \oa) + \gamma \sum_{\os'\in\ocS}\sum_{a'\in\ocA}\ov{P}(\os'|\os, \oa)\pi(\oa'|\os') Q(\os')
\end{align}
\end{definition}

\begin{lemma}[Properties of $\cT^\pi$]
\label{lemma:bellman_expectation_contraction}
$\cT^\pi$ is a $\gamma$-contraction and $Q^\pi$ is its unique fixed point.
\end{lemma}
\begin{proof}
The proof is the same as in the MDP case: let $\pi\in\Pi, Q_1, Q_2\in(\Real^2)^{\ocS\ocA}$.
\begin{align*}
    \forall \os\in\ocS, \oa\in\ocA,\quad \left|\cT^\pi Q_1(\os,\oa) - \cT^\pi Q_2(\os,\oa)\right| &= \left|\gamma\expectedvalueover{\substack{\os'\sim\ov{P}(\os'|\os,\oa) \\ \oa'\sim\pi(\oa'|\os')}} Q_1(\os',\oa') - Q_2(\os',\oa')\right|\\
    &\leq \gamma\left\|Q_1-Q_2\right\|_\infty
\end{align*}
Hence, $\left\|\cT^\pi Q_1  - \cT^\pi Q_2 \right\|_\infty \leq \gamma\left\|Q_1-Q_2\right\|_\infty$

By Banach fixed point theorem, $\cT^\pi$ admits a unique fixed point.
It can be easily verified that $Q^\pi$ is indeed this fixed point by combining the two Bellman Expectation equations \eqref{eq:bellman_expectation_V} and \eqref{eq:bellman_expectation_Q}.
\end{proof}

We now prove the result of \Cref{prop:greedy_optimal}.
\begin{proof}
Notice from the definitions of $\cT$ and $\cT^\pi$ in \eqref{eq:bellman-optimality} and \eqref{eq:bellman_exepctation_operator} that $\cT$ and $\cT^{\pi_\text{greedy}(\cdot;Q^*)}$ coincide on $Q^*$. Moreover, since $Q^* = \cT Q^*$ by \cref{thm:bellman-optimality}, we have: $    \cT^{\pi_\text{greedy}(\cdot;Q^*)} Q^* = \cT Q^* = Q^*
$.
Hence, $Q^*$ is a fixed point of $\cT^{\pi_\text{greedy}(\cdot;Q^*)}$, and by \cref{lemma:bellman_expectation_contraction} it must be equal to $Q^{\pi_\text{greedy}(\cdot;Q^*)}$

To show the same result for $V^*$, notice that 
\begin{equation*}
    V^{\pi_\text{greedy}(Q^*)}(\os) = \expectedvalueover{\oa\sim\pi_\text{greedy}(Q^*)}Q^{\pi_\text{greedy}(Q^*)}(\os,\oa) = \expectedvalueover{\oa\sim\pi_\text{greedy}(Q^*)}Q^*(\os,\oa)
\end{equation*}
By applying the definitions of $Q^*$ and $\pi_\text{greedy}$, we recover the definition of $V^*$.
\end{proof}

%\subsection{Lemma \ref{lemma:concavity}}

%\begin{proof}. Let $s,s'\in\cS, a\in\cA$.
%We first prove those results for $V_r^*(s', \cdot)$

%\textbf{Non-decreasing}

%Consider $\beta_a^1 \leq \beta_a^2 \in \cB$.
%Any policy that satisfies the budget $\beta_a^1$ in $s'$ also satisfies $\beta_a^2$, so $\Pi_c(s', \beta_a^1) \subset \Pi_c(s', \beta_a^2)$. Hence, by taking the max over policies, $V_r^*(s', \beta_a^1) \leq V_r^*(s', \beta_a^2)$.
%Hence, $V_r^*(s', \cdot)$ is non-decreasing.

%\textbf{Concave}

%By contradiction: assume that $V_r^*(s', \cdot)$ is not concave, i.e. there exist $\beta^1 < \beta^2\in \cB$ and $p\in(0, 1)$ such that $\beta^3 = (1-p)\beta^1 + p\beta^2$ verifies: $V_r^*(s', \beta^3) < (1-p)V_r^*(s', \beta^1) + pV_r^*(s',\beta^2)$. By definition of $V^*$, there must be $\pi_1,\pi_2\in\Pi^*$ such that $V^*(s', \beta^1) = V^{\pi_1}(s', \beta^1)$ and $V^*(s', \beta^2) = V^{\pi_2}(s', \beta^2)$. 

%Define $\pi = (1-p)(\pi_1(\cdot, \beta^1), \pi_1) + p(\pi_2(\cdot, \beta^2), \pi_2)$. By linearity of $V^\pi$ with respect to $\pi$, we have that $V_c^\pi(s', \beta^3) = (1-p)V_c^{\pi_1}(s', \beta^1) + pV_c^{\pi_2}(s', \beta^2) \leq (1-p)\beta^1 + p\beta^2 = \beta^3$ since $\pi_1, \pi_2\in\Pi^*(s')\subset\Pi_a(s')$, so $\pi$ respects the budget $\beta^3$. Moreover, we also have $V_r^\pi(s', \beta^3) = (1-p)V_r^{\pi_1}(s', \beta^1) + pV_r^{\pi_2}(s', \beta^2) > V_r^*(s', \beta^3)$, which contradicts the definition of $V_r^*$.

%Consequently, $V_r^*(s', \cdot)$ is non-decreasing and concave. By \eqref{eq:bellman_expectation_Q} we see that $Q_r^*(s,a,\cdot) = R_r(s,a) + \gamma\expectedvalueover{s'}V_r^*(s', \cdot)$  is too.


%\end{proof}

%\subsection{Lemma \ref{lemma:tau_concavity}}


%\subsection{Lemma \ref{lemma:pi_hull}}

%\td

%\begin{proof}
%If the estimates $q^c_0, q^c_1$ are accurate, then by construction and linearity of the expectation, the returned mixture policy has an expected total cost of $\expectedvalueover{a, \beta_a \sim\pi_\text{greedy}}Q_c(s, a, \beta_a) = \beta$ as desired in \eqref{eq:pi_greedy_constraint}. Because the $Q_r(s,a,\cdot)$ is concave and under its tangents, this mixture must have the largest $Q_r$ possible as required in \eqref{eq:pi_greedy_reward}. The special case of a tie $q_r^0 = q_r^1$ is considered, where we do minimise $Q_c$ as required in \eqref{eq:pi_greedy_cost}.
%\end{proof}


\subsection{Proposition \ref{prop:bftq_pi_hull}}
\begin{definition}
Let $A$ be a set, and $f$ a function defined on $A$. We define:

\begin{itemize}
    \item Convex hull of $A$: $\cC(A) = \{\sum_{i=1}^p \lambda_i a_i: a_i\in A, \lambda_i\in\Real^+, \sum_{i=1}^p \lambda_i = 1, p\in\Natural\}$
    \item Convex edges of $A$: $\cC^2(A) = \{\lambda a_1 + (1-\lambda)a_2: a_1, a_2\in A, \lambda\in[0, 1]\}$
    \item Dirac distributions of $A$: $\delta(A) = \{\delta(a-a_0): a_0\in A\}$ 
    \item Image of $A$ by $f$: $f(A) = \{f(a): a\in A\}$
\end{itemize}
\end{definition}

\begin{proof}
Let $\os=(s,\beta)\in\ocS$ and $Q\in(\Real^2)^{\ocS\ocA}$. We recall the definition of $\pi_\text{greedy}$:
\begin{subequations}
\begin{equation}
    \pi_\text{greedy}(\oa|\os; Q) \in \argmin_{\rho\in\Pi_r^Q} \expectedvalueover{\oa\sim\rho}Q_c(\os, \oa) \tag{\ref{eq:pi_greedy_cost}}
\end{equation}
\begin{align}
    \text{where }\quad\Pi_r^Q = &\argmax_{\rho\in\cM(\ocA)} \expectedvalueover{\oa\sim\rho} Q_r(\os, \oa) \tag{\ref{eq:pi_greedy_reward}}\\
    & \text{ s.t. }  \expectedvalueover{\oa\sim\rho} Q_c(\os, \oa) \leq \beta \tag{\ref{eq:pi_greedy_constraint}}
\end{align}
\end{subequations}

Note that any policy in the $\argmin$ in \eqref{eq:pi_greedy_cost} is suitable to compute $\cT$.
We first reduce the set of candidate optimal policies.
Consider the problem described in \eqref{eq:pi_greedy_reward},\eqref{eq:pi_greedy_constraint}: it can be seen as a single-step CMDP problem with reward $R_r=Q_r$ and cost $R_c=Q_c$. By \citep[Theorem 4.4][]{BEUTLER1985236}, we know that the solutions are mixtures of two deterministic policies. Hence, we can replace $\cM(\cA)$ by $\cC^2(\delta(\ocA))$ in \eqref{eq:pi_greedy_reward}.

Moreover, remark that:
\begin{align*}
    \{\expectedvalueover{\oa\sim\rho} Q(\os,\oa): \rho\in \cC^2(\delta(\ocA))\} &= \{\expectedvalueover{\oa\sim\rho} Q(\os,\oa): \rho=(1-\lambda)\delta(\oa-\oa_1)+\lambda\delta(\oa-\oa_2), \oa_1,\oa_2\in\ocA, \lambda\in[0,1]\} \\
    &= \{(1-\lambda)Q(\os, \oa_1)+\lambda Q(\os, \oa_2), \oa_1,\oa_2\in\ocA, \lambda\in[0,1]\} \\
    &= \cC^2(Q(\os,\ocA))\}
\end{align*}

Hence, the problem \eqref{eq:pi_greedy_reward}, \eqref{eq:pi_greedy_constraint} has become:
\begin{equation*}
    \tilde{\Pi}^Q_r = \argmax_{(q_r, q_c)\in\cC^2(Q(\os, \ocA))} q_r \quad\text{ s.t. }\quad q_c \leq \beta 
\end{equation*}
and the solution of $\pi_\text{greedy}$ is $q^*=\argmin_{q\in\tilde{\Pi}^Q_r} q_c$. 

The original problem in the space of actions $\ocA$ is now expressed in the space of values $Q(\os, \ocA)$ (which is why we use $=$ instead of $\in$ before $\argmin$ here).

We further restrict the search space of $q^*$ following two observations:
\begin{enumerate}
    \item $q^*$ belongs to the \emph{undominated} points $\cC^2(Q^-)$:
    \begin{align}
        \label{eq:q_minus_undominated}
        Q^+ &= \{(q_c, q_r): q_c > q_c^{\pm} = \min_{q^+} q_c^+\text{ s.t. }q^+\in\argmax_{q\in Q(\os,\ocA)} q_r\}\\
        Q^- &= Q(\os,\ocA) \setminus Q^+
    \end{align}
    Denote $q^*$ = $(1-\lambda) q^1 + \lambda q^2$, with $q^1, q^2\in Q(\os,\ocA)$. There are three possible cases:
    \begin{enumerate}
        \item $q^1, q^2 \not\in Q^-$. Then $q_c^* = (1-\lambda) q^1_c + \lambda q^2_c > q_c^{\pm}$. But then $q_c^{\pm} < q_c^* \leq \beta$ so $q^{\pm}\in\tilde{\Pi}^Q_r$ with a strictly lower $q_c$ than $q^*$, which contradicts the $\argmin$.
        \item $q^1\in Q^-, q^2 \not\in Q^-$. But then consider the mixture $q^\top = (1-\lambda) q^1 + \lambda q^\pm$. Since $q_r^{\pm} \geq q_r^{2}$ and $q_r^{\pm} < q_r^{2}$, we also have $q^\top_r \geq q_r^*$ and $q^\top_c < q_c^*$, which also contradicts the $\argmin$.
        \item $q^1,q^2\in Q^-$ is the only remaining possibility.
    \end{enumerate}
    \item $q^*$ belongs to the \emph{top frontier} $\cF$:
    \begin{equation*}
        \cF_Q = \{q\in \cC^2(Q^-): \not\exists q'\in \cC^2(Q^-): q_c=q_c'\text{ and }q_r<q_r'\}
    \end{equation*}
    Trivially, otherwise q' would be a better candidate than $q^*$.
\end{enumerate}


Let us characterise this frontier $\cF$. It is both:
\begin{enumerate}
    \item the \emph{graph of a non-decreasing function}: $\forall q^1, q^2\in\cF$ such that $q_c^1\leq q_c^2$ then $q_r^1\leq q_r^2$.\\
    By contradiction, if we had $q_r^1 > q_r^2$, we could define $q^\top = (1-\lambda)q^1 + \lambda q^\pm$ where $q^\pm$ is the dominant point as defined in \eqref{eq:q_minus_undominated}. By choosing $\lambda=(q^2_c-q^1_c)/(q^\pm_c-q^1_c)$ such that $q^\top_c = q_c^2$, then since $q_r^\pm \geq q_r^1 > q_r 2$ we also have $q^\top_r > q_r^2$ which contradicts $q^2\in\cF$.
    \item the \emph{graph of a concave function}: $\forall q^1, q^2, q^3\in\cF$ such that $q_c^1\leq q_c^2 \leq q_c^3$ with $\lambda$ such that $q^2_c = (1-\lambda)q^1_c + \lambda q^3_c$, then $q_r^2 \geq (1-\lambda)q_r^1 + \lambda q_r^3$.\\
    Trivially, otherwise the point $q^\top = (1-\lambda)q^1 + \lambda q^3$ would verify $q^\top_c=q^2_c$ and $q^\top_r > q^2_r$, which would contradict $q^2 \in\cF$.
\end{enumerate}

We denote $\cF_Q = \cF \cap Q$. Clearly, $q^*\in\cC^2(\cF_Q)$: let $q^1, q^2\in Q^-$ such that $q^* = (1-\lambda)q^1  + \lambda q^2$. First, $q^1, q^2\in Q^-\subset\cC^2(Q^-)$. Then, by contradiction, if there existed $q^{1'}$ or $q^{2'}$ with equal $q_c$ and strictly higher $q_r$, again we could build an admissible mixture $q^{\top}=(1-\lambda)q^{1'}  + \lambda q^{2'}$ strictly better than $q^*$.

$q^*$ can be written as $q^* = (1-\lambda)q^1  + \lambda q^2$ with $q^1, q^2\in\cF_Q$ and, without loss of generality, $q^1_c \leq q^2_c$.

\textbf{Regular case:} there exists $q^0\in\cF_Q$ such that $q^0_c \geq \beta$.

Then $q^1$ and $q^2$ must flank the budget: $q_c^1 \leq \beta \leq q_c^2$. Indeed, by contradiction, if $q_c^2 \geq q_c^1 > \beta$ then $q_c^* > \beta$ which contradicts $\Pi_r^Q$. Conversely, if $q_c^1 \leq q_c^2 < \beta$ then $q^* < \beta \leq q^0_c$, which would make $q^*$ a worse candidate than $q^\top=(1-\lambda)q^* + \lambda q^0$ when $\lambda$ is chosen such that $q_c^\top=\beta$, and contradict $\Pi_r^Q$ again.

Because $\cF$ is the graph of a non-decreasing function, $\lambda$ should be as high as possible, as long as the budget $q^*\leq\beta$ is respected. We reach the highest $q_r^*$ when $q^*_c=\beta$, that is: $\lambda=(\beta-q_c^1)/(q_c^2-q_c^1)$.

It remains to show that $q^1$ and $q^2$ are two successive points in $\cF_Q$: $\not\exists q\in\cF_Q\setminus\{q^1, q^2\}: q^1_c \leq q_c \leq q^2_c$. Otherwise, as $\cF$ is the graph of a concave function, we would have $q_r \geq (1-\mu)q_r^1 + \mu q_r^2$. $q_r$ cannot be strictly greater than $(1-\mu)q_r^1 + \mu q_r^2$ which would contradict $q^*$, but it can still be equal, which means the tree points $q, q^1, q^2$ are aligned. In fact, every points aligned with $q^1$ and $q^2$ can also be used to construct mixtures resulting in $q^*$, but among these solutions we can still choose $q^1$ and $q^2$ as the two points in $\cF_Q$ closest to $q^*$.

\textbf{Edge case:} $\forall q\in\cF_Q, q_c < \beta$. Then  $q^* =  \argmax_{q\in\cF} q_r = q^\pm =  \argmax_{q\in Q^-} q_r$

\end{proof}


%\begin{proof}
%First, a straightforward proof by induction shows that for all $k\in\Natural$, $Q_k$ computed at iteration $k$ of either Algorithm \ref{algo:bvi} or Algorithm \ref{algo:bftq} is concave non-decreasing with respect to $\beta_a$: the initialisation is trivial from $Q_0 = 0$, and the heredity stems from Lemma \ref{lemma:tau_concavity}.
%\end{proof}


% \subsection{Decomposition Lemma}

% \begin{lemma}
%     For any sequence real valued functions $f_1,\ldots,f_n$ and any real number $c$, we have:
%     \[
%         \begin{array}{lcl}
%             \underbrace{\max\limits_{\sum_i x_i \leq c}\sum_j f_j(x_j)}_{(a)} & \quad{}=\quad{} & \underbrace{\max\limits_{\sum_i c_i \leq c}\left(\sum_j\max\limits_{x\leq c_j} f_j(x)\right)}_{(b)}\\
%         \end{array}
%     \]
% \end{lemma}

% \begin{proof}
%     Let us first show that $(a)\leq(b)$.
%     By definition of the maximum on a set, for any $f_j$ and any $c_j$ we have:
%     $\max\limits_{x\leq c_j} f_j(x) \geq f_j(c_j)$.
%     Hence, by replacing these terms in $(b)$ we get:
%       \[
%     \begin{array}{lcl}
%         \max\limits_{\sum_i c_i \leq c} \sum_j f_j(c_j) & \quad{}\leq\quad{} & \max\limits_{\sum_i c_i\leq c}\left(\sum_j \max\limits_{x_j\leq c_j} f_j(x_j)\right)\\
%     \end{array}
%     \]
%     The left hand side of this inequality is just a rewriting of $(a)$ with different dummy variables names.

%     Let us show now that $(a) \geq (b)$.
%     Let $\hat{x}_1,\ldots,\hat{x}_n, \hat{c}_1, \ldots \hat{c}_n$ be a realisation (argmax) of $(b)$.
%     By definition of $(b)$'s feasible set, we have $\sum_i\hat{c}_i \leq c$ and for any $i$: $\hat{x}_i\leq \hat{c}_i$.
%     Because $\sum_i\hat{x}_i\leq \sum_i\hat{c}_i \leq c$, the tuple $(\hat{x}_1, \ldots \hat{x}_n)$ is also a feasible value for $(a)$. And, by definition of the maximum on a set: $(a) = \max\limits_{\sum_i x_i \leq c} \sum_j f_j(x_j) \geq \sum_j f_j(\hat{x}_j) = (b)$.
% \end{proof}

\section{Risk-Sensitive Exploration}

We recall the Risk-Sensitive Exploration algorithm in \cref{algo:risk-sensitive-exploration}


\input{source/risk-sensitive-explo-pseudo-code.tex}

\section{Scalable Implementation of BFTQ}
\label{sec:bftq-full}

We recall the scalable version of BFTQ in \cref{algo:bftq_full} and the architecture of the neural network \cref{fig:architecture}.

\begin{minipage}[t]{0.5\textwidth}
\begin{figure}[H]
    \centering
    \input{source/architecture.tex}
    \caption{Neural Network for $Q$-functions approximation when $\cS=\Real^2$ and $|\cA| = 2$.}
    \label{fig:architecture}
\end{figure}
\end{minipage}

\input{source/bftq-pseudo-code.tex}

\section{Experiments}

\subsection{The Lagrangian Relaxation Baseline}

As explained on \cref{fig:Lagrangian}, the optimal deterministic policy can be obtained by a line-search on the Lagrange multiplier values $\lambda$.

Then, according to \citet[Theorem 4.4]{BEUTLER1985236}, the optimal policy is a randomised mixture of two deterministic policies: the safest deterministic policy that violates the constraint $\pi_{\lambda-}$ and the riskier of the feasible ones $\pi_{\lambda+}$.

Fitted-Q (FTQ)~\citep{Ernst2005,Riedmiller2005} can be easily adapted for continuous states CMDP and BMDP through this methodology, but given the high variance it requires a lot of simulations to get a proper estimate of the calibration curve. Our purpose is to avoid this calibration phase.

\begin{figure}[tp]
    \centering
    \includegraphics[width=0.5\textwidth]{source/img/CalibrationExample}
    \caption{Calibration of a penalty multiplier according to the budget $\beta$. The optimal multiplier $\lambda^*_{\text{avg}}$ is the smallest one to satisfy the budget constraint on average. Safer policies can also be selected according to the largest deviation from this mean cost.}
    \label{fig:Lagrangian}
\end{figure}

\subsection{Environments Parameters}
\label{sec:env-parameters}

All environments parameters are displayed in \cref{tab:param-corridors},\cref{tab:param-slot-filling} and \cref{tab:param-highway-env}.

\paragraph{Remark on the slot-filling environment} When receiving an utterance, the system can either understand it $(\mu=\mu_u)$ or misunderstand it $(\mu=\mu_m)$ with a fixed probability called the sentence error rate $ser$. Then, the speech recognition score is simulated \citep{Khouzaimi2015}: $srs = (1+\exp(-x))^{-1}$ with $x\sim N(\mu, \sigma)$. It's the confidence score of the natural language understanding module about the last utterance. Note that here are no recognition errors ($ser=0$ and $srs=1$) when the user provides information using the numeric pad.

\begin{table}[ht!]
    \centering
    \begin{tabularx}{1.0\textwidth}{lll}
        \toprule
        Parameter & Description & Value\tabularnewline
        \midrule
        - & Size of the environment & 7 x 6\tabularnewline
        - & \makecell[l]{Standard deviation of the Gaussian \\noise applied to actions} & (0.25,0.25)\tabularnewline
        H & Episode duration & 9\tabularnewline
        \bottomrule
    \end{tabularx}
    \caption{Parameters of \texttt{Corridors}}
    \label{tab:param-corridors}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabularx}{1.0\textwidth}{lll}
        \toprule
        Parameter & Description & Value\tabularnewline
        \midrule
        ser & Sentence Error Rate & 0.6\tabularnewline
        $\mu_m$& Gaussian mean for misunderstanding & -0.25\tabularnewline
        $\mu_u$& Gaussian mean for understanding & 0.25\tabularnewline
        $\sigma$& Gaussian standard deviation & 0.6\tabularnewline
        $p$& Probability of hang-up & 0.25\tabularnewline
        H & Episode duration & 10\tabularnewline
        - & Number of slots & 3\tabularnewline
        \bottomrule
    \end{tabularx}
    \caption{Parameters of \texttt{Slot-Filling}}
    \label{tab:param-slot-filling}
\end{table}


\begin{table}[ht!]
    \centering
    \begin{tabularx}{1.0\textwidth}{lll}
        \toprule
        Parameter & Description & Value\tabularnewline
        \midrule
        $N_v$& Number of vehicles & 2 - 6\tabularnewline
        $\sigma_p$& Standard deviation of vehicles initial positions & 100 m\tabularnewline
        $\sigma_v$& Standard deviation of vehicles initial velocities & 3 m/s\tabularnewline
        H & Episode duration & 15 s\tabularnewline
        \bottomrule
    \end{tabularx}

    \caption{Parameters of \texttt{highway-env}}
    \label{tab:param-highway-env}
\end{table}

\subsection{Algorithm parameters}
\label{sec:algorithms-parameters}

All algorithm parameters are displayed in \cref{tab:param-algo-corridors},\cref{tab:param-algo-slot-filling} and \cref{tab:param-algo-highway-env}.

\begin{table}[tp]
    \centering
    \begin{tabularx}{1.0\textwidth}{lll}
        \toprule
        Parameters & BFTQ(risk-sensitive) & BFTQ(risk-neutral)\tabularnewline
        \midrule
        architecture & 256x128x64 & 256x128x64\tabularnewline
        regularisation & 0.001 & 0.001\tabularnewline
        activation & relu & relu\tabularnewline
        size beta encoder & 3 & 3\tabularnewline
        initialisation & xavier & xavier\tabularnewline
        loss function & L2 & L2\tabularnewline
        optimizer & adam & adam\tabularnewline
        learning rate & 0.001 & 0.001\tabularnewline
        epoch (NN) & 1000 & 5000\tabularnewline
        normalize reward & true & true\tabularnewline
        epoch (FTQ) & 12 & 12\tabularnewline
        & 0:0.01:1 & -\tabularnewline
        beta for duplication & 0:0.1:1 & 0:0.1:1\tabularnewline
        & (1,1) & (1,1)\tabularnewline
        & 5000 & 5000\tabularnewline
        & 10 & 10\tabularnewline
        & 4 & 4\tabularnewline
        & 1000 & 1000\tabularnewline
        decay epsilon scheduling & 0.001 & 0.001\tabularnewline
        \bottomrule

    \end{tabularx}
    \caption{Algorithms parameters for \texttt{Corridors}}
    \label{tab:param-algo-corridors}
\end{table}

\begin{table}[tp]
    \centering
    \begin{tabularx}{1.0\textwidth}{lll}
        \toprule
        Parameters & BFTQ & FTQ\tabularnewline
        \midrule
        architecture & 256x128x64 & 128x64x32\tabularnewline
        regularisation & 0.0005 & 0.0005\tabularnewline
        activation & relu & relu\tabularnewline
        size beta encoder & 50 & -\tabularnewline
        initialisation & xavier & xavier\tabularnewline
        loss function & L2 & L2\tabularnewline
        optimizer & adam & adam\tabularnewline
        learning rate & 0.001 & 0.001\tabularnewline
        epoch (NN) & 5000 & 5000\tabularnewline
        normalize reward & true & true\tabularnewline
        epoch (FTQ) & 11 & 11\tabularnewline
        & 0:0.01:1 & -\tabularnewline
        beta for duplication & - & -\tabularnewline
        & (1,1) & (1,1)\tabularnewline
        & 5000 & 5000\tabularnewline
        & 10 & 10\tabularnewline
        & 6 & 6\tabularnewline
        & 1000 & 1000\tabularnewline
        decay epsilon scheduling & 0.001 & 0.001\tabularnewline
        \bottomrule
    \end{tabularx}
    \caption{Algorithms parameters for \texttt{Slot-Filling}}
    \label{tab:param-algo-slot-filling}
\end{table}
%
\begin{table}[tp]
    \centering
    \begin{tabularx}{1.0\textwidth}{lll}
        \toprule
        Parameters & BFTQ & FTQ\tabularnewline
        \midrule
        architecture & 256x128x64 & 128x64x32\tabularnewline
        regularisation & 0.0005 & 0\tabularnewline
        activation & relu & relu\tabularnewline
        size beta encoder & 50 & -\tabularnewline
        initialisation & xavier & xavier\tabularnewline
        loss function & L2 & L2\tabularnewline
        optimizer & adam & adam\tabularnewline
        learning rate & 0.001 & 0.01\tabularnewline
        epoch (NN) & 5000 & 400\tabularnewline
        normalize reward & true & true\tabularnewline
        epoch (FTQ) & 15 & 15\tabularnewline
        & 0:0.01:1 & -\tabularnewline
        beta for duplication & - & -\tabularnewline
        & (0.9, 0.9) & (0.9, 0.9)\tabularnewline
        & 10000 & 10000\tabularnewline
        & 10 & 10\tabularnewline
        & 10 & 10\tabularnewline
        & 150 & 150\tabularnewline
        decay epsilon scheduling & 0.0003 & 0.0003\tabularnewline
        \bottomrule
    \end{tabularx}
    \caption{Algorithms parameters for \texttt{Highway-Env}}
    \label{tab:param-algo-highway-env}
\end{table}

\subsection{Examples of BFTQ policies executions}
\label{sec:bftq-executions}

In \cref{table:dialogues}, we display two dialogues done with the same BFTQ policy. The policy is given two budgets to respect in expectation,$\beta=0$ and $\beta=0.5$. For budget 0, one can see that the system never uses the \texttt{ask\_num\_pad} action. Instead, it uses \texttt{ask\_oral} , an action subject to recognition errors. The system keeps asking for the same slot 2, because it has the lowest speech recognition score. It eventually summarizes the form to the user, but then reaches the maximum dialogue length and thus faces a dialogue failure. For budget 0.5, the system first asks in a safe way, with \texttt{ask\_oral}. It may want to \texttt{ask\_num\_pad} if one of the speech recognition score is low. Then, the system proceeds to a confirmation of the slot values. If it is incorrect, the system continues the dialogue using unsafe the \texttt{ask\_num\_pad} action to be certain of the slot values.

\input{source/dialogues.tex}



% \clearpage
% \bibliographystyle{named}
% \bibliography{budgeted_rl}
% \end{document}

